{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Objective:** \n\nImplement a Retrieval-Augmented Generation (RAG) pipeline. The goal is to build an efficient system that can answer queries related to Indian laws by combining effective retrieval techniques with LLMs. You are encouraged to explore and implement retrieval optimization methods to enhance the system's performance in retrieving relevant legal content and ensuring accurate and contextually grounded responses.","metadata":{}},{"cell_type":"markdown","source":"1. Storing in VectorDB\n2. Retrieving from DB\n3. Implementing Retrieving with Langchain","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install chromadb sentence-transformers datasets langchain langchain-community langchain-core\n# pip install datasets           # For Hugging Face datasets\n# pip install sentence-transformers  # For embeddings\n# pip install chromadb           # Vector database\n# pip install langchain          # Orchestration & RAG\n# pip install openai             # For ChatOpenAI / OpenAI API\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:34:32.041243Z","iopub.execute_input":"2025-10-18T15:34:32.041539Z","iopub.status.idle":"2025-10-18T15:36:15.175627Z","shell.execute_reply.started":"2025-10-18T15:34:32.041516Z","shell.execute_reply":"2025-10-18T15:36:15.174778Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import chromadb\nimport os\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:36:15.176661Z","iopub.execute_input":"2025-10-18T15:36:15.176902Z","iopub.status.idle":"2025-10-18T15:36:41.724199Z","shell.execute_reply.started":"2025-10-18T15:36:15.176880Z","shell.execute_reply":"2025-10-18T15:36:41.723597Z"}},"outputs":[{"name":"stderr","text":"2025-10-18 15:36:28.829075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760801789.010406      57 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760801789.066615      57 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"configurator = {\n  \"dataset\": {\n    \"name\": \"mratanusarkar/Indian-Laws\",\n    \"split\": \"train\"\n  },\n  \"embedding\": {\n    \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\"\n  },\n  \"chroma\": {\n    \"collection_name\": \"indian_laws_collection\",\n    \"persist_directory\": \"./chroma_store\",\n    \"batch_size\": 5000\n  },\n  \"llm\": {\n    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n    \"temperature\": 0,\n    \"api_key\": \"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\"\n  },\n  \"retriever\": {\n    \"top_k\": 3\n  },\n  \"logging\": {\n    \"level\": \"INFO\",\n    \"format\": \"%(asctime)s - %(levelname)s - %(message)s\"\n  }\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:36:41.725684Z","iopub.execute_input":"2025-10-18T15:36:41.726210Z","iopub.status.idle":"2025-10-18T15:36:41.730569Z","shell.execute_reply.started":"2025-10-18T15:36:41.726190Z","shell.execute_reply":"2025-10-18T15:36:41.729879Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nLoad and expose configuration variables from config.json\n\"\"\"\n\nimport json\nimport os\nimport logging\n\n# Path to config.json\n# CONFIG_PATH = os.path.join(os.path.dirname(__file__), \"config.json\")\n\ndef load_config():\n    \"\"\"Load and validate configuration file.\"\"\"\n    try:\n        with open(CONFIG_PATH, \"r\") as f:\n            return json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Configuration file not found at {CONFIG_PATH}\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON format in config file: {str(e)}\")\n\n# Load the config\nconfig = configurator\n\n# ------------------------------\n# Dataset\n# ------------------------------\nDATASET_NAME = config[\"dataset\"][\"name\"]\nDATASET_SPLIT = config[\"dataset\"][\"split\"]\n\n# ------------------------------\n# Embedding model\n# ------------------------------\nEMBEDDING_MODEL_NAME = config[\"embedding\"][\"model_name\"]\n\n# ------------------------------\n# Chroma settings\n# ------------------------------\nCHROMA_COLLECTION_NAME = config[\"chroma\"][\"collection_name\"]\nCHROMA_PERSIST_DIR = config[\"chroma\"][\"persist_directory\"]\nCHROMA_BATCH_SIZE = config[\"chroma\"].get(\"batch_size\", 5000)\n\n# ------------------------------\n# LLM settings\n# ------------------------------\nLLM_MODEL_NAME = config[\"llm\"][\"model_name\"]\nLLM_TEMPERATURE = config[\"llm\"][\"temperature\"]\nOPENAI_API_KEY = config[\"llm\"][\"api_key\"]\n\n# ------------------------------\n# Retriever settings\n# ------------------------------\nTOP_K = config[\"retriever\"].get(\"top_k\", 3)\n\n# ------------------------------\n# Logging setup\n# ------------------------------\nlogging.basicConfig(\n    level=getattr(logging, config[\"logging\"][\"level\"], logging.INFO),\n    format=config[\"logging\"][\"format\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:36:41.731140Z","iopub.execute_input":"2025-10-18T15:36:41.731363Z","iopub.status.idle":"2025-10-18T15:36:41.746800Z","shell.execute_reply.started":"2025-10-18T15:36:41.731338Z","shell.execute_reply":"2025-10-18T15:36:41.746115Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import kaggle_secrets\nimport os\nimport wandb\n\n# Fetch API key from Kaggle Secrets\nOPENAI_API_KEY = kaggle_secrets.UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n\n# Set as environment variable\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:37:18.107095Z","iopub.execute_input":"2025-10-18T15:37:18.107681Z","iopub.status.idle":"2025-10-18T15:37:18.202147Z","shell.execute_reply.started":"2025-10-18T15:37:18.107657Z","shell.execute_reply":"2025-10-18T15:37:18.201593Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# !rm -rf /kaggle/working/*\nos.listdir(\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:25:00.440824Z","iopub.execute_input":"2025-10-18T13:25:00.441429Z","iopub.status.idle":"2025-10-18T13:25:00.446072Z","shell.execute_reply.started":"2025-10-18T13:25:00.441407Z","shell.execute_reply":"2025-10-18T13:25:00.445457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nConfig-driven RAG Pipeline with Dataset Ingestion\n- Loads Hugging Face dataset\n- Stores embeddings in ChromaDB\n- Fully modular LangChain RAG\n\"\"\"\n\nimport logging\nfrom typing import List\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n\n# from config import (\n#     DATASET_NAME, DATASET_SPLIT, EMBEDDING_MODEL_NAME,\n#     CHROMA_COLLECTION_NAME, CHROMA_PERSIST_DIR, CHROMA_BATCH_SIZE,\n#     LLM_MODEL_NAME, LLM_TEMPERATURE, OPENAI_API_KEY, TOP_K\n# )\n\n# ------------------------------\n# Logging\n# ------------------------------\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# ------------------------------\n# Dataset loading & preprocessing\n# ------------------------------\ndef load_dataset_texts(dataset_name: str = DATASET_NAME, split: str = DATASET_SPLIT):\n    \"\"\"\n    Load Hugging Face dataset and combine act_title, section, and law into one text per row.\n    Returns:\n        texts (List[str]): List of combined text documents\n        metadatas (List[dict]): List of metadata dictionaries per document\n    \"\"\"\n    try:\n        dataset = load_dataset(dataset_name, split=split)\n        texts = []\n        metadatas = []\n\n        for row in dataset:\n            if row.get(\"law\"):  # skip empty laws\n                doc_text = f\"Act: {row['act_title']}\\nSection: {row['section']}\\nLaw: {row['law']}\"\n                texts.append(doc_text)\n                metadatas.append({\"act_title\": row[\"act_title\"], \"section\": row[\"section\"]})\n\n        logging.info(f\"Loaded {len(texts)} documents from dataset '{dataset_name}' (split={split})\")\n        return texts, metadatas\n\n    except Exception as e:\n        logging.error(f\"Failed to load dataset '{dataset_name}': {str(e)}\")\n        raise\n\n\n# ------------------------------\n# ChromaDB ingestion\n# ------------------------------\nimport uuid\n\ndef store_embeddings_in_chroma(texts: List[str], metadatas: List[dict]):\n    \"\"\"\n    Store embeddings in ChromaDB in batches with unique IDs and metadata.\n    \n    Args:\n        texts (List[str]): List of combined text documents\n        metadatas (List[dict]): List of metadata dictionaries corresponding to each document\n    \"\"\"\n    try:\n        if len(texts) != len(metadatas):\n            raise ValueError(\"Length of texts and metadatas must be the same.\")\n\n        # Initialize embedding model\n        embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n        \n        # Connect or create Chroma collection\n        client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n        try:\n            collection = client.get_collection(CHROMA_COLLECTION_NAME)\n        except:\n            collection = client.create_collection(CHROMA_COLLECTION_NAME)\n        \n        logging.info(f\"Storing embeddings in ChromaDB (collection={CHROMA_COLLECTION_NAME})...\")\n\n        # Store in batches\n        for i in range(0, len(texts), CHROMA_BATCH_SIZE):\n            batch_texts = texts[i:i+CHROMA_BATCH_SIZE]\n            batch_metadatas = metadatas[i:i+CHROMA_BATCH_SIZE]\n            \n            # Generate embeddings for the batch\n            embeddings = embed_model.encode(batch_texts, show_progress_bar=True)\n\n            # Generate unique IDs for each document\n            ids = [str(uuid.uuid4()) for _ in batch_texts]\n\n            # Add documents, embeddings, and metadata to Chroma\n            collection.add(\n                ids=ids,\n                documents=batch_texts,\n                embeddings=embeddings,\n                metadatas=batch_metadatas\n            )\n\n            logging.info(f\"Stored batch {i//CHROMA_BATCH_SIZE + 1} ({len(batch_texts)} docs)\")\n\n        logging.info(\"All embeddings and metadata stored successfully.\")\n\n    except Exception as e:\n        logging.error(f\"Failed to store embeddings: {str(e)}\")\n        raise\n\n\"\"\"\nEnhanced LangChain RAG pipeline (Step 2: LLM Interaction Enhancements)\n\nFeatures added:\n- System prompt enrichment (explicit citation rules, conservative behavior)\n- Inline citation guidance (ask model to use [1], [2] style referring to retrieved contexts)\n- Automatic answer evaluation: use the LLM to score faithfulness (0-100) + short rationale\n- Show numbered retrieved contexts so citations map to sources\n\nPrereqs:\n- config.py must expose: OPENAI_API_KEY, LLM_MODEL_NAME, LLM_TEMPERATURE, TOP_K,\n  CHROMA_COLLECTION_NAME, CHROMA_PERSIST_DIR, EMBEDDING_MODEL_NAME\n- Chroma must be populated with documents and metadata\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Tuple, Dict, Any\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n\n# # Import your config variables (assumes config.py in same package)\n# from config import (\n#     OPENAI_API_KEY,\n#     LLM_MODEL_NAME,\n#     LLM_TEMPERATURE,\n#     TOP_K,\n#     CHROMA_COLLECTION_NAME,\n#     CHROMA_PERSIST_DIR,\n#     EMBEDDING_MODEL_NAME,\n# )\n\n# Basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n\n# ------------------------------\n# Build RAG chain with enriched system prompt\n# ------------------------------\ndef build_rag_chain(top_k: int = TOP_K) -> RetrievalQA:\n    \"\"\"\n    Build LangChain RetrievalQA pipeline using ChromaDB with ChatOpenAI.\n    The system prompt enforces:\n      - Use ONLY the provided context to answer.\n      - When referring to a retrieved chunk, cite it inline using [1], [2], ... which correspond\n        to the order of retrieved documents returned by the retriever.\n      - If the answer cannot be supported by the context, respond: \"I don't know based on the available data.\"\n      - Provide a short final answer (2-4 sentences) and then a one-line \"Sources:\" list with numbers and metadata.\n    \"\"\"\n    # Prepare vectorstore retriever\n    embed_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n    vectorstore = Chroma(\n        collection_name=CHROMA_COLLECTION_NAME,\n        persist_directory=CHROMA_PERSIST_DIR,\n        embedding_function=embed_model,\n    )\n    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\n    # Ensure API key in environment (LangChain sometimes validates env var)\n    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\n    # Create ChatOpenAI instance and pass api key explicitly for safety\n    llm = ChatOpenAI(\n        model_name=LLM_MODEL_NAME,\n        temperature=LLM_TEMPERATURE,\n        openai_api_key=OPENAI_API_KEY,  # explicit param avoids validation errors in some versions\n    )\n\n    # Enriched system prompt with citation instructions and behaviour constraints\n    system_prompt = (\n        \"You are a precise legal assistant whose job is to answer user legal questions using only the provided context. \"\n        \"BEHAVIOR RULES:\\n\"\n        \"1) Use ONLY the information present in the 'Context' to produce answers. Do not invent facts.\\n\"\n        \"2) If you assert any fact that appears in the context, append an inline citation to the corresponding chunk using [1], [2], ... where numbers match the order of retrieved contexts.\\n\"\n        \"   Example: 'Under Section 420, cheating is punishable [2].'\\n\"\n        \"3) If the information needed is NOT present in the context, respond exactly: \\\"I don't know based on the available data.\\\" and avoid speculation.\\n\"\n        \"4) Provide a concise answer (2-4 sentences). After the answer, add a short 'Sources:' line listing used source numbers and, if available, metadata (e.g., Act, Section).\\n\"\n        \"5) If multiple sources conflict, state that there are conflicting sources and list them.\\n\"\n        \"6) Maintain neutral, formal tone appropriate for legal information. This is NOT legal advice.\"\n    )\n\n    # The human template must include both {context} and {question} variables\n    human_template = \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nRespond with citations and a Sources: line.\"\n\n    prompt = ChatPromptTemplate.from_messages([\n        SystemMessagePromptTemplate.from_template(system_prompt),\n        HumanMessagePromptTemplate.from_template(human_template),\n    ])\n\n    # Build RetrievalQA with chain_type=\"stuff\" and pass our custom prompt\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type=\"stuff\",\n        chain_type_kwargs={\"prompt\": prompt},\n    )\n\n    logging.info(\"RAG pipeline built with enriched system prompt.\")\n    return qa_chain\n\n# ------------------------------\n# Utility to safely extract text from ChatOpenAI response\n# ------------------------------\ndef extract_llm_output(resp: Any) -> str:\n    \"\"\"\n    Safely extract the text from a ChatOpenAI response.\n    Handles AIMessage, list of AIMessage, dicts, and plain strings.\n    \"\"\"\n    if isinstance(resp, list) and hasattr(resp[0], \"content\"):\n        return resp[0].content\n    elif hasattr(resp, \"content\"):\n        return resp.content\n    elif isinstance(resp, dict):\n        if \"text\" in resp:\n            return resp[\"text\"]\n        elif \"generations\" in resp:\n            return resp[\"generations\"][0][\"message\"][\"content\"]\n    return str(resp)\n\n# ------------------------------\n# Answer evaluation helper\n# ------------------------------\ndef evaluate_answer(answer: str, sources: List[Any], question: str) -> Dict[str, Any]:\n    \"\"\"\n    Use the LLM to score the answer's faithfulness to the provided sources.\n\n    Returns a dict:\n        {\n          \"score\": int (0-100),\n          \"verdict\": \"faithful\"|\"not_faithful\"|\"partial\",\n          \"comment\": \"short explanation\"\n        }\n\n    Note: This will make an extra API call. Keep responses short to reduce cost.\n    \"\"\"\n    # Build a compact context summarizing sources for the evaluator\n    # We'll provide numbered source blocks so the evaluator can compare.\n    numbered_sources = []\n    for i, src in enumerate(sources, start=1):\n        # src is typically a Document with .page_content and .metadata\n        metadata = src.metadata if hasattr(src, \"metadata\") else {}\n        preview = src.page_content[:1000].replace(\"\\n\", \" \")\n        numbered_sources.append(f\"[{i}] Meta: {metadata}\\n{preview}\")\n\n    eval_context = \"\\n\\n\".join(numbered_sources)\n\n    # Create a short evaluation prompt\n    eval_system = (\n        \"You are an evaluator that judges whether a given ANSWER is faithful to the provided SOURCES. \"\n        \"Give a score from 0 to 100 (100 = fully faithful and supported by sources, 0 = not at all supported). \"\n        \"Then provide a one-line verdict: faithful / partial / not_faithful, and a two-sentence rationale.\"\n    )\n\n    eval_user = (\n        f\"SOURCES:\\n{eval_context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\\n{answer}\\n\\n\"\n        \"Please provide a JSON object with keys: score (int 0-100), verdict (string), comment (string).\"\n    )\n\n    # Use the same ChatOpenAI but create a short-lived instance (we can reuse OPENAI settings)\n    evaluator = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0.0, openai_api_key=OPENAI_API_KEY)\n    # We will pass messages directly\n    from langchain.schema import SystemMessage, HumanMessage\n\n    # Prepare messages properly\n    messages = [\n        SystemMessage(content=eval_system),\n        HumanMessage(content=eval_user)\n    ]\n\n    # Call the evaluator\n    resp = evaluator(messages)\n    # resp may be a string; try to parse JSON out of it. Keep parsing robust.\n    # raw = resp.content if hasattr(resp, \"content\") else str(resp)\n    # raw = resp['generations'][0]['message']['content']\n    raw = extract_llm_output(resp)\n    # raw = resp\n    # Try to extract JSON - but keep fallback to a simple heuristic\n    import json\n    try:\n        # Sometimes model outputs JSON directly; attempt parsing\n        parsed = json.loads(raw.strip())\n        # Ensure keys exist\n        score = int(parsed.get(\"score\", 0))\n        verdict = parsed.get(\"verdict\", \"unknown\")\n        comment = parsed.get(\"comment\", \"\")\n    except Exception:\n        # Fallback: do a simple heuristic parse (look for numbers)\n        # This is a best-effort fallback; you can refine parsing/prompting for strict JSON output.\n        score = 0\n        verdict = \"unknown\"\n        comment = raw.strip().replace(\"\\n\", \" \")[:300]\n\n    return {\"score\": score, \"verdict\": verdict, \"comment\": comment}\n\n\n# ------------------------------\n# Interactive loop with context display and evaluation\n# ------------------------------\ndef interactive_loop_with_evaluation(qa_chain: RetrievalQA, evaluate: bool = True):\n    \"\"\"\n    Interactive loop that:\n      - runs the QA chain,\n      - displays retrieved numbered contexts (so [1],[2] correspond),\n      - prints the LLM answer,\n      - optionally runs evaluation of the answer's faithfulness.\n    \"\"\"\n    print(\"RAG (enhanced) ready. Type your legal query (or 'exit' to quit):\")\n    while True:\n        query_text = input(\">> \").strip()\n        if query_text.lower() in [\"exit\", \"quit\"]:\n            print(\"Exiting...\")\n            break\n        try:\n            # Run chain - it returns answer and source_documents\n            result = qa_chain(query_text)\n            answer = result.get(\"result\")\n            sources = result.get(\"source_documents\", [])\n\n            # Print numbered retrieved contexts so model citations align with these indices\n            print(\"\\n===== Retrieved Contexts (numbered) =====\\n\")\n            for i, src in enumerate(sources, start=1):\n                preview = src.page_content[:600].replace(\"\\n\", \" \")\n                meta = src.metadata if hasattr(src, \"metadata\") else {}\n                print(f\"[{i}] {preview} ...\")\n                if meta:\n                    print(f\"    Metadata: {meta}\")\n                print(\"-\" * 80)\n\n            # Show answer\n            print(\"\\n===== Answer =====\\n\")\n            print(answer)\n\n            # Optionally evaluate\n            if evaluate:\n                eval_result = evaluate_answer(answer, sources, query_text)\n                print(\"\\n===== Answer Evaluation =====\")\n                print(f\"Score: {eval_result['score']} / 100\")\n                print(f\"Verdict: {eval_result['verdict']}\")\n                print(f\"Comment: {eval_result['comment']}\")\n                print(\"-\" * 80)\n\n        except Exception as e:\n            print(\"Error:\", e)\n            logging.exception(\"Error during RAG query.\")\n\n\n# ------------------------------\n# Top-level runner\n# ------------------------------\ndef run_rag_pipeline():\n    \"\"\"\n    Build RAG chain and run interactive loop with evaluation.\n    Assumes Chroma has been ingested already. If not, ingest separately before running.\n    \"\"\"\n    # Load dataset\n    # texts, metadatas = load_dataset_texts()\n    # # Store embeddings\n    # store_embeddings_in_chroma(texts, metadatas)\n    # Build RAG\n    qa_chain = build_rag_chain()\n    # Interactive loop\n    interactive_loop_with_evaluation(qa_chain, evaluate=True)\n\n\nif __name__ == \"__main__\":\n    run_rag_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T16:44:16.781844Z","iopub.execute_input":"2025-10-18T16:44:16.782569Z","iopub.status.idle":"2025-10-18T16:44:34.862893Z","shell.execute_reply.started":"2025-10-18T16:44:16.782543Z","shell.execute_reply":"2025-10-18T16:44:34.862095Z"}},"outputs":[{"name":"stdout","text":"RAG (enhanced) ready. Type your legal query (or 'exit' to quit):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">>   How many years of punishment is for murder attempt.\n"},{"name":"stdout","text":"\n===== Retrieved Contexts (numbered) =====\n\n[1] Act: Air Force Act, 1950 Section: 67 Law: 67. Attempt.- Any person subject to this Act who attempts to commit any of the offences specified in sections 34 to 66 inclusive, and in such attempt does any act towards the commission of the offence shall, on conviction by court-martial, where no express provision is made by this Act for the punishment of such attempt, be liable, if the offence attempted to be committed is punishable with death, to suffer imprisonment for a term which may extend to fourteen years or such less punishment as is in this Act mentioned; and if the offence attempted to be  ...\n    Metadata: {'act_title': 'Air Force Act, 1950', 'section': '67'}\n--------------------------------------------------------------------------------\n[2] Act: Army Act, 1950 Section: 65 Law: 65. Attempt.- Any person subject to this Act who attempts to commit any of the offences specified in sections 34 to 64 inclusive and in such attempt does any act towards the commission of the offence, shall, on conviction by court-martial, where no express provision is made by this Act for the punishment of such attempt, be liable, if the offence attempted to be committed is punishable with death, to suffer imprisonment for a term which may extend to fourteen years or such less punishment as is in this Act mentioned; and if the offence attempted to be commi ...\n    Metadata: {'act_title': 'Army Act, 1950', 'section': '65'}\n--------------------------------------------------------------------------------\n[3] Act: Border Security Force Act, 1968 Section: 42 Law: 42. Attempt. Any person subject to this Act who attempts to commit any of the offences specified in sections 14 to 41 (both inclusive) and in such attempt does any act towards the commission of the offence shall, on conviction by a Security Force Court, where no express provision is made by this Act for the punishment of such attempt, be liable, (a) if the offence attempted to be committed is punishable with death, to suffer imprisonment for a term which may extend to fourteen years or such less punishment as is in this Act mentioned; and ( ...\n    Metadata: {'section': '42', 'act_title': 'Border Security Force Act, 1968'}\n--------------------------------------------------------------------------------\n\n===== Answer =====\n\nThe punishment for an attempt to commit murder, which is typically punishable with death, can extend to fourteen years of imprisonment under the Air Force Act, 1950 [1], the Army Act, 1950 [2], and the Border Security Force Act, 1968 [3]. \n\nSources: [1] Air Force Act, 1950, Section 67; [2] Army Act, 1950, Section 65; [3] Border Security Force Act, 1968, Section 42.\n\n===== Answer Evaluation =====\nScore: 100 / 100\nVerdict: faithful\nComment: The answer accurately reflects the information provided in the sources regarding the punishment for an attempt to commit murder, which is punishable with death and can extend to fourteen years of imprisonment under the specified acts. All relevant sources are correctly cited to support the claim.\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">>  exit\n"},{"name":"stdout","text":"Exiting...\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# import os\n# print(os.path.abspath(CHROMA_PERSIST_DIR))\n# print(os.access(CHROMA_PERSIST_DIR, os.W_OK))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:26:54.404852Z","iopub.status.idle":"2025-10-18T13:26:54.405073Z","shell.execute_reply.started":"2025-10-18T13:26:54.404966Z","shell.execute_reply":"2025-10-18T13:26:54.404976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # !mkdir -p ./chroma_store\n# # !chmod 755 ./chroma_store\n# >>  How many years of punishment is for murder attempt.\n\n# ===== Answer =====\n\n# The punishment for an attempt to commit murder, which is typically punishable with death, can extend to fourteen years of imprisonment under the Air Force Act, Army Act, and Border Security Force Act.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T13:26:54.405787Z","iopub.status.idle":"2025-10-18T13:26:54.406107Z","shell.execute_reply.started":"2025-10-18T13:26:54.405943Z","shell.execute_reply":"2025-10-18T13:26:54.405959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ### WORKING VERSION: \n# \"\"\"\n# Config-driven RAG Pipeline with Dataset Ingestion\n# - Loads Hugging Face dataset\n# - Stores embeddings in ChromaDB\n# - Fully modular LangChain RAG\n# \"\"\"\n\n# import logging\n# from typing import List\n# from datasets import load_dataset\n# from sentence_transformers import SentenceTransformer\n# import chromadb\n# from chromadb.utils import embedding_functions\n# from langchain.chains import RetrievalQA\n# from langchain.vectorstores import Chroma\n# from langchain.embeddings import SentenceTransformerEmbeddings\n\n# # from config import (\n# #     DATASET_NAME, DATASET_SPLIT, EMBEDDING_MODEL_NAME,\n# #     CHROMA_COLLECTION_NAME, CHROMA_PERSIST_DIR, CHROMA_BATCH_SIZE,\n# #     LLM_MODEL_NAME, LLM_TEMPERATURE, OPENAI_API_KEY, TOP_K\n# # )\n\n# # ------------------------------\n# # Logging\n# # ------------------------------\n# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# # ------------------------------\n# # Dataset loading & preprocessing\n# # ------------------------------\n# def load_dataset_texts(dataset_name: str = DATASET_NAME, split: str = DATASET_SPLIT):\n#     \"\"\"\n#     Load Hugging Face dataset and combine act_title, section, and law into one text per row.\n#     Returns:\n#         texts (List[str]): List of combined text documents\n#         metadatas (List[dict]): List of metadata dictionaries per document\n#     \"\"\"\n#     try:\n#         dataset = load_dataset(dataset_name, split=split)\n#         texts = []\n#         metadatas = []\n\n#         for row in dataset:\n#             if row.get(\"law\"):  # skip empty laws\n#                 doc_text = f\"Act: {row['act_title']}\\nSection: {row['section']}\\nLaw: {row['law']}\"\n#                 texts.append(doc_text)\n#                 metadatas.append({\"act_title\": row[\"act_title\"], \"section\": row[\"section\"]})\n\n#         logging.info(f\"Loaded {len(texts)} documents from dataset '{dataset_name}' (split={split})\")\n#         return texts, metadatas\n\n#     except Exception as e:\n#         logging.error(f\"Failed to load dataset '{dataset_name}': {str(e)}\")\n#         raise\n\n\n# # ------------------------------\n# # ChromaDB ingestion\n# # ------------------------------\n# import uuid\n\n# def store_embeddings_in_chroma(texts: List[str], metadatas: List[dict]):\n#     \"\"\"\n#     Store embeddings in ChromaDB in batches with unique IDs and metadata.\n    \n#     Args:\n#         texts (List[str]): List of combined text documents\n#         metadatas (List[dict]): List of metadata dictionaries corresponding to each document\n#     \"\"\"\n#     try:\n#         if len(texts) != len(metadatas):\n#             raise ValueError(\"Length of texts and metadatas must be the same.\")\n\n#         # Initialize embedding model\n#         embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n        \n#         # Connect or create Chroma collection\n#         client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n#         try:\n#             collection = client.get_collection(CHROMA_COLLECTION_NAME)\n#         except:\n#             collection = client.create_collection(CHROMA_COLLECTION_NAME)\n        \n#         logging.info(f\"Storing embeddings in ChromaDB (collection={CHROMA_COLLECTION_NAME})...\")\n\n#         # Store in batches\n#         for i in range(0, len(texts), CHROMA_BATCH_SIZE):\n#             batch_texts = texts[i:i+CHROMA_BATCH_SIZE]\n#             batch_metadatas = metadatas[i:i+CHROMA_BATCH_SIZE]\n            \n#             # Generate embeddings for the batch\n#             embeddings = embed_model.encode(batch_texts, show_progress_bar=True)\n\n#             # Generate unique IDs for each document\n#             ids = [str(uuid.uuid4()) for _ in batch_texts]\n\n#             # Add documents, embeddings, and metadata to Chroma\n#             collection.add(\n#                 ids=ids,\n#                 documents=batch_texts,\n#                 embeddings=embeddings,\n#                 metadatas=batch_metadatas\n#             )\n\n#             logging.info(f\"Stored batch {i//CHROMA_BATCH_SIZE + 1} ({len(batch_texts)} docs)\")\n\n#         logging.info(\"All embeddings and metadata stored successfully.\")\n\n#     except Exception as e:\n#         logging.error(f\"Failed to store embeddings: {str(e)}\")\n#         raise\n\n\n\n# from langchain.chat_models import ChatOpenAI\n\n# # ------------------------------\n# # LangChain RAG Pipeline using ChatOpenAI\n# # ------------------------------\n# def build_rag_chain(top_k: int = TOP_K):\n#     \"\"\"\n#     Build LangChain RetrievalQA pipeline using ChromaDB with ChatOpenAI.\n    \n#     Args:\n#         top_k (int): Number of documents to retrieve per query\n    \n#     Returns:\n#         RetrievalQA: Configured RAG chain\n#     \"\"\"\n#     # Embeddings for vectorstore\n#     embed_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n#     vectorstore = Chroma(\n#         collection_name=CHROMA_COLLECTION_NAME,\n#         persist_directory=CHROMA_PERSIST_DIR,\n#         embedding_function=embed_model\n#     )\n#     retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\n#     # Set environment variable to avoid validation errors\n#     import os\n#     os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\n#     # Use ChatOpenAI instead of OpenAI\n#     llm = ChatOpenAI(\n#         model_name=LLM_MODEL_NAME,  # e.g., \"gpt-4o-mini\"\n#         temperature=LLM_TEMPERATURE\n#     )\n\n#     qa_chain = RetrievalQA.from_chain_type(\n#         llm=llm,\n#         retriever=retriever,\n#         return_source_documents=True,\n#         chain_type=\"stuff\"\n#     )\n#     logging.info(\"RAG pipeline ready with ChatOpenAI.\")\n#     return qa_chain\n\n# # ------------------------------\n# # Run interactive query loop\n# # ------------------------------\n# def interactive_loop(qa_chain: RetrievalQA):\n#     \"\"\"\n#     Interactive CLI for querying the RAG pipeline.\n#     \"\"\"\n#     print(\"RAG pipeline ready. Type your query (or 'exit' to quit):\")\n#     while True:\n#         query_text = input(\">> \").strip()\n#         if query_text.lower() in [\"exit\", \"quit\"]:\n#             print(\"Exiting...\")\n#             break\n#         try:\n#             result = qa_chain(query_text)\n#             answer = result.get(\"result\")\n#             sources = result.get(\"source_documents\", [])\n#             print(\"\\n===== Answer =====\\n\")\n#             print(answer)\n#             # print(\"\\n===== Source Documents =====\\n\")\n#             # for i, src in enumerate(sources, 1):\n#             #     print(f\"{i}. {src.page_content}\")\n#             #     print(f\"Metadata: {src.metadata}\")\n#             #     print(\"-\"*60)\n#         except Exception as e:\n#             print(\"Error;\", e)\n#             logging.error(f\"Error during RAG query: {str(e)}\")\n\n# # ------------------------------\n# # Main function\n# # ------------------------------\n# def run_rag_pipeline():\n#     # Load dataset\n#     # texts, metadatas = load_dataset_texts()\n#     # # Store embeddings\n#     # store_embeddings_in_chroma(texts, metadatas)\n#     # Build RAG\n#     qa_chain = build_rag_chain()\n#     # # Interactive loop\n#     interactive_loop(qa_chain)\n\n# if __name__ == \"__main__\":\n#     run_rag_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:51:09.423524Z","iopub.execute_input":"2025-10-18T15:51:09.424088Z","iopub.status.idle":"2025-10-18T15:51:09.430941Z","shell.execute_reply.started":"2025-10-18T15:51:09.424065Z","shell.execute_reply":"2025-10-18T15:51:09.430064Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ## WORKING V2: Added system prompt to enhance LLM and its worked \n# >>  How many years of punishment is for murder attempt.\n# ===== Answer =====\n# Based on the provided context, if the offence attempted to be committed is punishable with death (such as murder), the punishment for an attempt is imprisonment for a term which may extend to fourteen years.\n# >>  How many years of punishment is for murder attempt. Just provide me precise number\n# ===== Answer =====\n# Up to fourteen years.\n# >>  What is the section code for doing second marriage? And what is the punishment for it.\n# ===== Answer =====\n# The section code for doing a second marriage while already married under the Special Marriage Act, 1954 is Section 43. The punishment for this offense is that the person shall be deemed to have committed an offense under section 494 or section 495 of the Indian Penal Code, and the marriage solemnized shall be void\n\n\n# \"\"\"\n# Config-driven RAG Pipeline with Dataset Ingestion\n# - Loads Hugging Face dataset\n# - Stores embeddings in ChromaDB\n# - Fully modular LangChain RAG\n# \"\"\"\n\n# import logging\n# from typing import List\n# from datasets import load_dataset\n# from sentence_transformers import SentenceTransformer\n# import chromadb\n# from chromadb.utils import embedding_functions\n# from langchain.chains import RetrievalQA\n# from langchain.vectorstores import Chroma\n# from langchain.embeddings import SentenceTransformerEmbeddings\n\n# # from config import (\n# #     DATASET_NAME, DATASET_SPLIT, EMBEDDING_MODEL_NAME,\n# #     CHROMA_COLLECTION_NAME, CHROMA_PERSIST_DIR, CHROMA_BATCH_SIZE,\n# #     LLM_MODEL_NAME, LLM_TEMPERATURE, OPENAI_API_KEY, TOP_K\n# # )\n\n# # ------------------------------\n# # Logging\n# # ------------------------------\n# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# # ------------------------------\n# # Dataset loading & preprocessing\n# # ------------------------------\n# def load_dataset_texts(dataset_name: str = DATASET_NAME, split: str = DATASET_SPLIT):\n#     \"\"\"\n#     Load Hugging Face dataset and combine act_title, section, and law into one text per row.\n#     Returns:\n#         texts (List[str]): List of combined text documents\n#         metadatas (List[dict]): List of metadata dictionaries per document\n#     \"\"\"\n#     try:\n#         dataset = load_dataset(dataset_name, split=split)\n#         texts = []\n#         metadatas = []\n\n#         for row in dataset:\n#             if row.get(\"law\"):  # skip empty laws\n#                 doc_text = f\"Act: {row['act_title']}\\nSection: {row['section']}\\nLaw: {row['law']}\"\n#                 texts.append(doc_text)\n#                 metadatas.append({\"act_title\": row[\"act_title\"], \"section\": row[\"section\"]})\n\n#         logging.info(f\"Loaded {len(texts)} documents from dataset '{dataset_name}' (split={split})\")\n#         return texts, metadatas\n\n#     except Exception as e:\n#         logging.error(f\"Failed to load dataset '{dataset_name}': {str(e)}\")\n#         raise\n\n\n# # ------------------------------\n# # ChromaDB ingestion\n# # ------------------------------\n# import uuid\n\n# def store_embeddings_in_chroma(texts: List[str], metadatas: List[dict]):\n#     \"\"\"\n#     Store embeddings in ChromaDB in batches with unique IDs and metadata.\n    \n#     Args:\n#         texts (List[str]): List of combined text documents\n#         metadatas (List[dict]): List of metadata dictionaries corresponding to each document\n#     \"\"\"\n#     try:\n#         if len(texts) != len(metadatas):\n#             raise ValueError(\"Length of texts and metadatas must be the same.\")\n\n#         # Initialize embedding model\n#         embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n        \n#         # Connect or create Chroma collection\n#         client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n#         try:\n#             collection = client.get_collection(CHROMA_COLLECTION_NAME)\n#         except:\n#             collection = client.create_collection(CHROMA_COLLECTION_NAME)\n        \n#         logging.info(f\"Storing embeddings in ChromaDB (collection={CHROMA_COLLECTION_NAME})...\")\n\n#         # Store in batches\n#         for i in range(0, len(texts), CHROMA_BATCH_SIZE):\n#             batch_texts = texts[i:i+CHROMA_BATCH_SIZE]\n#             batch_metadatas = metadatas[i:i+CHROMA_BATCH_SIZE]\n            \n#             # Generate embeddings for the batch\n#             embeddings = embed_model.encode(batch_texts, show_progress_bar=True)\n\n#             # Generate unique IDs for each document\n#             ids = [str(uuid.uuid4()) for _ in batch_texts]\n\n#             # Add documents, embeddings, and metadata to Chroma\n#             collection.add(\n#                 ids=ids,\n#                 documents=batch_texts,\n#                 embeddings=embeddings,\n#                 metadatas=batch_metadatas\n#             )\n\n#             logging.info(f\"Stored batch {i//CHROMA_BATCH_SIZE + 1} ({len(batch_texts)} docs)\")\n\n#         logging.info(\"All embeddings and metadata stored successfully.\")\n\n#     except Exception as e:\n#         logging.error(f\"Failed to store embeddings: {str(e)}\")\n#         raise\n\n\n\n# from langchain.chat_models import ChatOpenAI\n\n# # ------------------------------\n# # LangChain RAG Pipeline using ChatOpenAI\n# # ------------------------------\n# import os\n# import logging\n# from langchain.chat_models import ChatOpenAI\n# from langchain.chains import RetrievalQA\n# from langchain.vectorstores import Chroma\n# from langchain.embeddings import SentenceTransformerEmbeddings\n# from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n\n# # ------------------------------\n# # LangChain RAG Pipeline using ChatOpenAI\n# # ------------------------------\n# def build_rag_chain(top_k: int = TOP_K):\n#     \"\"\"\n#     Build LangChain RetrievalQA pipeline using ChromaDB with ChatOpenAI and system prompt.\n    \n#     Args:\n#         top_k (int): Number of documents to retrieve per query\n    \n#     Returns:\n#         RetrievalQA: Configured RAG chain\n#     \"\"\"\n#     # Embeddings for vectorstore\n#     embed_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n#     vectorstore = Chroma(\n#         collection_name=CHROMA_COLLECTION_NAME,\n#         persist_directory=CHROMA_PERSIST_DIR,\n#         embedding_function=embed_model\n#     )\n#     retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n\n#     # Ensure API key is available\n#     os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\n#     # LLM model\n#     llm = ChatOpenAI(`\n#         model_name=LLM_MODEL_NAME,  # e.g., \"gpt-4o-mini\"\n#         temperature=LLM_TEMPERATURE\n#     )\n\n#     # ------------------------------\n#     # Add System Prompt for robust RAG behavior\n#     # ------------------------------\n#     system_prompt = (\n#         \"You are an intelligent assistant specialized in retrieving and reasoning \"\n#         \"over factual information from the provided context. \"\n#         \"Use only the retrieved documents (context) to answer accurately. \"\n#         \"If the answer is not found in the context, clearly say \"\n#         \"'I don't know based on the available data.' \"\n#         \"Always provide concise, factual, and context-grounded answers.\"\n#     )\n\n#     prompt = ChatPromptTemplate.from_messages([\n#         SystemMessagePromptTemplate.from_template(system_prompt),\n#         HumanMessagePromptTemplate.from_template(\n#             \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n#         )\n#     ])\n\n#     # Build RAG Chain\n#     qa_chain = RetrievalQA.from_chain_type(\n#         llm=llm,\n#         retriever=retriever,\n#         return_source_documents=True,\n#         chain_type=\"stuff\",\n#         chain_type_kwargs={\"prompt\": prompt}\n#     )\n\n#     logging.info(\"RAG pipeline ready with ChatOpenAI.\")\n#     return qa_chain\n\n\n# # ------------------------------\n# # Run interactive query loop\n# # ------------------------------\n# def interactive_loop(qa_chain: RetrievalQA):\n#     \"\"\"\n#     Interactive CLI for querying the RAG pipeline.\n#     \"\"\"\n#     print(\"RAG pipeline ready. Type your query (or 'exit' to quit):\")\n#     while True:\n#         query_text = input(\">> \").strip()\n#         if query_text.lower() in [\"exit\", \"quit\"]:\n#             print(\"Exiting...\")\n#             break\n#         try:\n#             result = qa_chain(query_text)\n#             answer = result.get(\"result\")\n#             sources = result.get(\"source_documents\", [])\n            \n#             # üîç Show retrieved context chunks before answer\n#             print(\"\\n===== Retrieved Context =====\\n\")\n#             for i, src in enumerate(sources, 1):\n#                 preview = src.page_content[:400].replace(\"\\n\", \" \")\n#                 print(f\"{i}. {preview}...\")\n#                 if src.metadata:\n#                     print(f\"   Metadata: {src.metadata}\")\n#                 print(\"-\" * 60)\n                \n#             print(\"\\n===== Answer =====\\n\")\n#             print(answer)\n#             # print(\"\\n===== Source Documents =====\\n\")\n#             # for i, src in enumerate(sources, 1):\n#             #     print(f\"{i}. {src.page_content}\")\n#             #     print(f\"Metadata: {src.metadata}\")\n#             #     print(\"-\"*60)\n#         except Exception as e:\n#             print(\"Error;\", e)\n#             logging.error(f\"Error during RAG query: {str(e)}\")\n\n# # ------------------------------\n# # Main function\n# # ------------------------------\n# def run_rag_pipeline():\n#     # Load dataset\n#     # texts, metadatas = load_dataset_texts()\n#     # # Store embeddings\n#     # store_embeddings_in_chroma(texts, metadatas)\n#     # Build RAG\n#     qa_chain = build_rag_chain()\n#     # # Interactive loop\n#     interactive_loop(qa_chain)\n\n# if __name__ == \"__main__\":\n#     run_rag_pipeline()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}